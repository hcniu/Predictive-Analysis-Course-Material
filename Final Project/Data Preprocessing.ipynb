{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Check for missing value\n",
    "The data is clean. **No feature contains missing value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features contain missing values.\n"
     ]
    }
   ],
   "source": [
    "result=train.isnull().sum()\n",
    "print('{} features contain missing values.'.format(len(result[result!=0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Outlier Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With boxplot, we notice that the **features are all rougly bell-shaped and have some obvious outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc4bdba1e90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apparently there are outliers within the data.\n",
    "#Besides, the features are all roughly bell-shaped\n",
    "train[['var_0','var_1','var_2','var_3','var_4']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check, I use the Fisher-Pearson coefficint to test the skewness of the variables. Eventually, **all features are roughly bell-shaped**. **All features' skewness coefficients are close to 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric columns</th>\n",
       "      <th>skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>var_44</td>\n",
       "      <td>-0.340170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>var_93</td>\n",
       "      <td>-0.238089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>var_81</td>\n",
       "      <td>-0.232524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>var_80</td>\n",
       "      <td>-0.220234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>var_86</td>\n",
       "      <td>-0.216857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>var_163</td>\n",
       "      <td>0.234778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var_0</td>\n",
       "      <td>0.235637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>var_179</td>\n",
       "      <td>0.242889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>var_2</td>\n",
       "      <td>0.260312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>var_168</td>\n",
       "      <td>0.267412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    numeric columns  skewness\n",
       "44           var_44 -0.340170\n",
       "93           var_93 -0.238089\n",
       "81           var_81 -0.232524\n",
       "80           var_80 -0.220234\n",
       "86           var_86 -0.216857\n",
       "..              ...       ...\n",
       "163         var_163  0.234778\n",
       "0             var_0  0.235637\n",
       "179         var_179  0.242889\n",
       "2             var_2  0.260312\n",
       "168         var_168  0.267412\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use Fisher-Pearson coefficient of skewness to address continuous variables' distributions:\n",
    "# =0 normally distributed\n",
    "# >0 positively skewed\n",
    "# <0 negatively skewed\n",
    "from scipy import stats\n",
    "numeric_cols=train.columns[2:]\n",
    "skewness=[]\n",
    "for i in numeric_cols:\n",
    "    result=stats.skew(train[i],bias=True)\n",
    "    skewness.append(result)\n",
    "skewness_tbl=pd.DataFrame({'numeric columns':numeric_cols,\n",
    "                           'skewness':skewness})\n",
    "skewness_tbl.sort_values(by='skewness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the features are **roughly normal distributed**, we use **Z-score larger than 3 or smaller than -3 to filter outliers**. Hence, **as long as one feature's value is considered outliers, the entire row is removed.** In the end, **only 5.52%** of data is considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After excluding the outliers, 5.52% of data is removed.\n"
     ]
    }
   ],
   "source": [
    "#Remove those rows of outliers\n",
    "train_no_outliers=train[(np.abs(stats.zscore(train.iloc[:,2:]))< 3).all(axis=1)]\n",
    "print('After excluding the outliers, {}% of data is removed.'.format(round((1-(len(train_no_outliers)/len(train)))*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_5</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4763</td>\n",
       "      <td>-2.3182</td>\n",
       "      <td>12.6080</td>\n",
       "      <td>8.6264</td>\n",
       "      <td>10.9621</td>\n",
       "      <td>3.5609</td>\n",
       "      <td>4.5322</td>\n",
       "      <td>15.2255</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.3068</td>\n",
       "      <td>6.6025</td>\n",
       "      <td>5.2912</td>\n",
       "      <td>0.4403</td>\n",
       "      <td>14.9452</td>\n",
       "      <td>1.0314</td>\n",
       "      <td>-3.6241</td>\n",
       "      <td>9.7670</td>\n",
       "      <td>12.5809</td>\n",
       "      <td>-4.7602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>train_199995</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4880</td>\n",
       "      <td>-0.4956</td>\n",
       "      <td>8.2622</td>\n",
       "      <td>3.5142</td>\n",
       "      <td>10.3404</td>\n",
       "      <td>11.6081</td>\n",
       "      <td>5.6709</td>\n",
       "      <td>15.1516</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1415</td>\n",
       "      <td>13.2305</td>\n",
       "      <td>3.9901</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>18.0249</td>\n",
       "      <td>-1.7939</td>\n",
       "      <td>2.1661</td>\n",
       "      <td>8.5326</td>\n",
       "      <td>16.6660</td>\n",
       "      <td>-17.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>train_199996</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>-2.4484</td>\n",
       "      <td>16.7052</td>\n",
       "      <td>6.6345</td>\n",
       "      <td>8.3096</td>\n",
       "      <td>-10.5628</td>\n",
       "      <td>5.8802</td>\n",
       "      <td>21.5940</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9611</td>\n",
       "      <td>4.6549</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>1.8341</td>\n",
       "      <td>22.2717</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>-2.1651</td>\n",
       "      <td>6.7419</td>\n",
       "      <td>15.9054</td>\n",
       "      <td>0.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>train_199997</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2232</td>\n",
       "      <td>-5.0518</td>\n",
       "      <td>10.5127</td>\n",
       "      <td>5.6456</td>\n",
       "      <td>9.3410</td>\n",
       "      <td>-5.4086</td>\n",
       "      <td>4.5555</td>\n",
       "      <td>21.5571</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0651</td>\n",
       "      <td>5.4414</td>\n",
       "      <td>3.1032</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>23.5311</td>\n",
       "      <td>-1.5736</td>\n",
       "      <td>1.2832</td>\n",
       "      <td>8.7155</td>\n",
       "      <td>13.8329</td>\n",
       "      <td>4.1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>train_199998</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7148</td>\n",
       "      <td>-8.6098</td>\n",
       "      <td>13.6104</td>\n",
       "      <td>5.7930</td>\n",
       "      <td>12.5173</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>6.0479</td>\n",
       "      <td>17.0152</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>8.6587</td>\n",
       "      <td>2.7337</td>\n",
       "      <td>11.1178</td>\n",
       "      <td>20.4158</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>10.0342</td>\n",
       "      <td>15.5289</td>\n",
       "      <td>-13.9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>train_199999</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8762</td>\n",
       "      <td>-5.7105</td>\n",
       "      <td>12.1183</td>\n",
       "      <td>8.0328</td>\n",
       "      <td>11.5577</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>5.2839</td>\n",
       "      <td>15.2058</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9842</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>15.2101</td>\n",
       "      <td>-2.4907</td>\n",
       "      <td>-2.2342</td>\n",
       "      <td>8.1857</td>\n",
       "      <td>12.1284</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188969 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "0            train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607   \n",
       "2            train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825   \n",
       "3            train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846   \n",
       "4            train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   \n",
       "5            train_5       0  11.4763 -2.3182  12.6080  8.6264  10.9621   \n",
       "...              ...     ...      ...     ...      ...     ...      ...   \n",
       "199995  train_199995       0  11.4880 -0.4956   8.2622  3.5142  10.3404   \n",
       "199996  train_199996       0   4.9149 -2.4484  16.7052  6.6345   8.3096   \n",
       "199997  train_199997       0  11.2232 -5.0518  10.5127  5.6456   9.3410   \n",
       "199998  train_199998       0   9.7148 -8.6098  13.6104  5.7930  12.5173   \n",
       "199999  train_199999       0  10.8762 -5.7105  12.1183  8.0328  11.5577   \n",
       "\n",
       "          var_5   var_6    var_7  ...  var_190  var_191  var_192  var_193  \\\n",
       "0       -9.2834  5.1187  18.6266  ...   4.4354   3.9642   3.1364   1.6910   \n",
       "2       -9.0837  6.9427  14.6155  ...   2.9057   9.7905   1.6704   1.6858   \n",
       "3       -1.8361  5.8428  14.9250  ...   4.4666   4.7433   0.7178   1.4214   \n",
       "4        2.4486  5.9405  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942   \n",
       "5        3.5609  4.5322  15.2255  ...  -6.3068   6.6025   5.2912   0.4403   \n",
       "...         ...     ...      ...  ...      ...      ...      ...      ...   \n",
       "199995  11.6081  5.6709  15.1516  ...   6.1415  13.2305   3.9901   0.9388   \n",
       "199996 -10.5628  5.8802  21.5940  ...   4.9611   4.6549   0.6998   1.8341   \n",
       "199997  -5.4086  4.5555  21.5571  ...   4.0651   5.4414   3.1032   4.8793   \n",
       "199998   0.5339  6.0479  17.0152  ...   2.6840   8.6587   2.7337  11.1178   \n",
       "199999   0.3488  5.2839  15.2058  ...   8.9842   1.6893   0.1276   0.3766   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0       18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "2       21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3       23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4       13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "5       14.9452   1.0314  -3.6241   9.7670  12.5809  -4.7602  \n",
       "...         ...      ...      ...      ...      ...      ...  \n",
       "199995  18.0249  -1.7939   2.1661   8.5326  16.6660 -17.8661  \n",
       "199996  22.2717   1.7337  -2.1651   6.7419  15.9054   0.3388  \n",
       "199997  23.5311  -1.5736   1.2832   8.7155  13.8329   4.1995  \n",
       "199998  20.4158  -0.0786   6.7980  10.0342  15.5289 -13.9001  \n",
       "199999  15.2101  -2.4907  -2.2342   8.1857  12.1284   0.1385  \n",
       "\n",
       "[188969 rows x 202 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_no_outliers.iloc[:,2:],train_no_outliers['target'], test_size=0.2, random_state=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Imbalance Data Issue: SMOTE\n",
    "We notice that the data is extremely imbalanced. Therefore, we will **use both undersampling and oversampling** to fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive cases(Class 1) only take account for 9.89% of the entire train data.\n"
     ]
    }
   ],
   "source": [
    "perc=y_train.value_counts()[1]/(y_train.value_counts()[1]+y_train.value_counts()[0])*100\n",
    "print('The positive cases(Class 1) only take account for {}% of the entire train data.'.format(round(perc,2)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    136218\n",
       "1     14957\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE on train data\n",
    "smt=SMOTE()\n",
    "x_train_smt, y_train_smt = smt.fit_resample(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE, there are totally 136218 positive cases and 136218 negative cases.\n",
      "The positive cases(Class 1) only take account for 50.0% of the entire train data.\n"
     ]
    }
   ],
   "source": [
    "perc=y_train_smt.value_counts()[1]/(y_train_smt.value_counts()[1]+y_train_smt.value_counts()[0])*100\n",
    "print('After SMOTE, there are totally {} positive cases and {} negative cases.'.format(y_train_smt.value_counts()[1],y_train_smt.value_counts()[0]))\n",
    "print('The positive cases(Class 1) only take account for {}% of the entire train data.'.format(round(perc,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression with grid-search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. XGBoost with grid-search CV  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
